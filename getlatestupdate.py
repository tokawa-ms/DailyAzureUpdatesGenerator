#!/usr/bin/env python3
"""
Azure Updates RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã€æŒ‡å®šæ™‚é–“å†…ã®æ›´æ–°ã‚’ Azure OpenAI ã§è¦ç´„ã™ã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

ç’°å¢ƒå¤‰æ•°:
- AOAI_ENDPOINT: Azure OpenAI ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
- AOAI_KEY: Azure OpenAI API ã‚­ãƒ¼
- DEPLOYMENT: Azure OpenAI ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå
- API_VER: Azure OpenAI API ãƒãƒ¼ã‚¸ãƒ§ãƒ³
- CHECK_HOURS: ãƒã‚§ãƒƒã‚¯å¯¾è±¡æ™‚é–“ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 24æ™‚é–“ï¼‰
"""

import os
import sys
import logging
import argparse
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional
import json
import time

# ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import feedparser
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("azure_updates.log", encoding="utf-8"),
        logging.StreamHandler(sys.stdout),
    ],
)
logger = logging.getLogger(__name__)


class AzureOpenAIClient:
    """Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    def __init__(self, endpoint: str, api_key: str, deployment: str, api_version: str):
        """
        Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–

        Args:
            endpoint: Azure OpenAI ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
            api_key: API ã‚­ãƒ¼
            deployment: ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå
            api_version: API ãƒãƒ¼ã‚¸ãƒ§ãƒ³
        """
        self.endpoint = endpoint.rstrip("/")
        self.api_key = api_key
        self.deployment = deployment
        self.api_version = api_version

        # HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨­å®šï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰
        self.session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

        # ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®š
        self.session.headers.update(
            {"Content-Type": "application/json", "api-key": self.api_key}
        )

        logger.info("Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")

    def summarize_update(
        self, title: str, description: str, link: str
    ) -> Optional[str]:
        """
        Azure Update ã‚’æ—¥æœ¬èªã§è¦ç´„

        Args:
            title: ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã®ã‚¿ã‚¤ãƒˆãƒ«
            description: ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã®è©³ç´°
            link: ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã®ãƒªãƒ³ã‚¯

        Returns:
            è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
        """
        try:
            url = f"{self.endpoint}/openai/deployments/{self.deployment}/chat/completions?api-version={self.api_version}"

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
            prompt = f"""
ä»¥ä¸‹ã®Azure Updateã‚’æ—¥æœ¬èªã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚æŠ€è¡“è€…å‘ã‘ã«é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’å«ã‚ã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {title}

è©³ç´°: {description}

ãƒªãƒ³ã‚¯: {link}

è¦ç´„ã¯ä»¥ä¸‹ã®å½¢å¼ã§ä½œæˆã—ã¦ãã ã•ã„ï¼š
- ä½•ãŒæ›´æ–°ã•ã‚ŒãŸã‹
- ä¸»ãªå¤‰æ›´ç‚¹ã‚„æ–°æ©Ÿèƒ½
- å½±éŸ¿ã‚’å—ã‘ã‚‹å¯¾è±¡
- æ³¨æ„ç‚¹ãŒã‚ã‚Œã°è¨˜è¼‰

200æ–‡å­—ç¨‹åº¦ã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
"""

            payload = {
                "messages": [
                    {
                        "role": "system",
                        "content": "ã‚ãªãŸã¯Azureã®å°‚é–€å®¶ã§ã™ã€‚Azure Updateã®å†…å®¹ã‚’æŠ€è¡“è€…å‘ã‘ã«åˆ†ã‹ã‚Šã‚„ã™ãæ—¥æœ¬èªã§è¦ç´„ã—ã¾ã™ã€‚",
                    },
                    {"role": "user", "content": prompt},
                ],
                "max_tokens": 500,
                "temperature": 0.3,
                "top_p": 0.95,
            }

            response = self.session.post(url, json=payload, timeout=30)
            response.raise_for_status()

            result = response.json()

            if "choices" in result and len(result["choices"]) > 0:
                summary = result["choices"][0]["message"]["content"].strip()
                logger.info(f"è¦ç´„å®Œäº†: {title[:50]}...")
                return summary
            else:
                logger.error("äºˆæœŸã—ãªã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã‚’å—ä¿¡ã—ã¾ã—ãŸ")
                return None

        except requests.exceptions.RequestException as e:
            logger.error("API ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            return None
        except Exception as e:
            logger.error("è¦ç´„å‡¦ç†ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            return None

    def __del__(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if hasattr(self, "session"):
            self.session.close()


class AzureUpdatesProcessor:
    """Azure Updates RSS ãƒ•ã‚£ãƒ¼ãƒ‰å‡¦ç†ã‚¯ãƒ©ã‚¹"""

    # Azure Updates ã®è¤‡æ•°ã®RSS URLã‚’è©¦è¡Œ
    RSS_URLS = [
        "https://www.microsoft.com/releasecommunications/api/v2/azure/rss",
        "https://azurecomcdn.azureedge.net/en-us/updates/feed/",
        "https://azure.microsoft.com/en-us/updates/feed/",
    ]

    def __init__(self, openai_client: AzureOpenAIClient, check_hours: int = 24):
        """
        ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’åˆæœŸåŒ–

        Args:
            openai_client: Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
            check_hours: ãƒã‚§ãƒƒã‚¯å¯¾è±¡æ™‚é–“ï¼ˆæ™‚é–“ï¼‰
        """
        self.openai_client = openai_client
        self.check_hours = check_hours
        self.cutoff_time = datetime.now(timezone.utc) - timedelta(hours=check_hours)

        logger.info(f"ãƒã‚§ãƒƒã‚¯å¯¾è±¡æ™‚é–“: {check_hours}æ™‚é–“ä»¥å†…")
        logger.info(f"ã‚«ãƒƒãƒˆã‚ªãƒ•æ™‚é–“: {self.cutoff_time}")

    def fetch_rss_feed(self) -> Optional[feedparser.FeedParserDict]:
        """
        RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆè¤‡æ•°URLã‚’è©¦è¡Œï¼‰

        Returns:
            ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
        """
        for url in self.RSS_URLS:
            try:
                logger.info("RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ä¸­")

                # HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è¨­å®šã—ã¦ã‚ˆã‚Šå …ç‰¢ã«ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—
                session = requests.Session()
                retry_strategy = Retry(
                    total=2,
                    backoff_factor=1,
                    status_forcelist=[429, 500, 502, 503, 504],
                )
                adapter = HTTPAdapter(max_retries=retry_strategy)
                session.mount("http://", adapter)
                session.mount("https://", adapter)

                # ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®š
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                    "Accept": "application/rss+xml, application/xml, text/xml, */*",
                    "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
                    "Accept-Encoding": "gzip, deflate, br",
                    "Connection": "keep-alive",
                    "Cache-Control": "no-cache",
                }

                # ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³çµŒç”±ã§å–å¾—
                feed = None
                try:
                    response = session.get(url, headers=headers, timeout=30)
                    response.raise_for_status()

                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å†…å®¹ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                    logger.debug(f"HTTP ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status_code}")
                    logger.debug(
                        f"Content-Type: {response.headers.get('Content-Type', 'Unknown')}"
                    )
                    logger.debug(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹é•·: {len(response.content)} bytes")

                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã§ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                    if not response.content:
                        logger.warning("RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒç©ºã§ã™")
                        continue

                    # æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¤œå‡ºãƒ»ãƒ‡ã‚³ãƒ¼ãƒ‰
                    content_bytes = response.content
                    logger.debug(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹æœ€åˆã®100ãƒã‚¤ãƒˆ: {content_bytes[:100]}")

                    # ã¾ãšContent-Typeãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºèª
                    content_type = response.headers.get("Content-Type", "")
                    encoding = "utf-8"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

                    if "charset=" in content_type:
                        try:
                            encoding = (
                                content_type.split("charset=")[1].split(";")[0].strip()
                            )
                            logger.debug(
                                f"Content-Typeã‹ã‚‰æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {encoding}"
                            )
                        except:
                            logger.debug(
                                "Content-Typeã‹ã‚‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºå¤±æ•—ã€UTF-8ã‚’ä½¿ç”¨"
                            )

                    # è¤‡æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦è¡Œï¼ˆBOMå¯¾å¿œã‚’å„ªå…ˆï¼‰
                    content_str = None
                    tried_encodings = [
                        "utf-8-sig",
                        encoding,
                        "utf-8",
                        "latin1",
                        "cp1252",
                    ]

                    for enc in tried_encodings:
                        try:
                            content_str = content_bytes.decode(enc)
                            logger.debug(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° '{enc}' ã§ãƒ‡ã‚³ãƒ¼ãƒ‰æˆåŠŸ")
                            break
                        except UnicodeDecodeError as e:
                            logger.debug(
                                f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° '{enc}' ã§ãƒ‡ã‚³ãƒ¼ãƒ‰å¤±æ•—: {e}"
                            )
                            continue

                    if content_str is None:
                        # æœ€å¾Œã®æ‰‹æ®µï¼šã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ã—ã¦ãƒ‡ã‚³ãƒ¼ãƒ‰
                        content_str = content_bytes.decode("utf-8", errors="replace")
                        logger.warning(
                            "ã™ã¹ã¦ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒå¤±æ•—ã€ã‚¨ãƒ©ãƒ¼ç„¡è¦–ã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã—ãŸ"
                        )

                    content_stripped = content_str.strip()

                    # BOMï¼ˆByte Order Markï¼‰ã‚’é™¤å»
                    if content_stripped.startswith("\ufeff"):
                        content_stripped = content_stripped[1:]
                        logger.debug("UTF-8 BOM ã‚’æ¤œå‡ºãƒ»é™¤å»ã—ã¾ã—ãŸ")

                    # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å…ˆé ­ã‚’è©³ã—ãç¢ºèª
                    logger.debug(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…ˆé ­50æ–‡å­—: '{content_stripped[:50]}'")
                    logger.debug(
                        f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…ˆé ­ã®ãƒã‚¤ãƒˆè¡¨ç¾: {content_stripped[:50].encode('unicode_escape')}"
                    )

                    # å„æ¡ä»¶ã‚’å€‹åˆ¥ã«ãƒã‚§ãƒƒã‚¯ã—ã¦ãƒ­ã‚°å‡ºåŠ›
                    starts_with_xml_header = content_stripped.startswith("<?xml")
                    starts_with_rss = content_stripped.startswith("<rss")
                    starts_with_feed = content_stripped.startswith("<feed")
                    starts_with_angle = content_stripped.startswith("<")

                    logger.debug(f"XMLå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯çµæœ:")
                    logger.debug(
                        f"  - XMLãƒ˜ãƒƒãƒ€ãƒ¼ ('<?xml') ã§é–‹å§‹: {starts_with_xml_header}"
                    )
                    logger.debug(f"  - RSSã‚¿ã‚° ('<rss') ã§é–‹å§‹: {starts_with_rss}")
                    logger.debug(f"  - Feedã‚¿ã‚° ('<feed') ã§é–‹å§‹: {starts_with_feed}")
                    logger.debug(f"  - ä»»æ„ã®ã‚¿ã‚° ('<') ã§é–‹å§‹: {starts_with_angle}")

                    # XMLã¨ã—ã¦å¦¥å½“ãã†ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šå¯›å®¹ã«ï¼‰
                    is_xml_like = (
                        starts_with_xml_header
                        or starts_with_rss
                        or starts_with_feed
                        or starts_with_angle
                    )

                    logger.debug(f"æœ€çµ‚åˆ¤å®š is_xml_like: {is_xml_like}")

                    if not is_xml_like:
                        logger.warning("XMLã§ã¯ãªã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹")
                        logger.warning(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…ˆé ­200æ–‡å­—: {content_str[:200]}")
                        logger.warning(
                            f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹å…ˆé ­ã®ãƒã‚¤ãƒˆ: {content_str[:50].encode('unicode_escape')}"
                        )
                        continue

                    logger.info(f"XMLãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆç¢ºèªOK: {content_stripped[:100]}...")

                    # feedparser ã§ãƒ‘ãƒ¼ã‚¹
                    logger.debug("feedparser ã«ã‚ˆã‚‹ãƒ‘ãƒ¼ã‚¹é–‹å§‹...")
                    feed = feedparser.parse(response.content)
                    logger.debug(
                        f"feedparser ãƒ‘ãƒ¼ã‚¹å®Œäº†: bozo={getattr(feed, 'bozo', 'Unknown')}"
                    )

                except requests.exceptions.RequestException as e:
                    logger.warning(f"HTTP ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {type(e).__name__}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: feedparser ã®ç›´æ¥å–å¾—ã‚’è©¦è¡Œ
                    logger.info("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: feedparser ã§ã®ç›´æ¥å–å¾—ã‚’è©¦è¡Œ")
                    feed = feedparser.parse(url)
                    logger.debug("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ feedparser ãƒ‘ãƒ¼ã‚¹å®Œäº†")

                finally:
                    session.close()

                # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ‘ãƒ¼ã‚¹çµæœã‚’ãƒã‚§ãƒƒã‚¯
                logger.debug(
                    f"ãƒ•ã‚£ãƒ¼ãƒ‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç¢ºèª: {type(feed)} - hasattr(feed, 'entries'): {hasattr(feed, 'entries')}"
                )

                if not feed:
                    logger.warning("ãƒ•ã‚£ãƒ¼ãƒ‰ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    continue

                # ã‚¨ãƒ³ãƒˆãƒªã®å­˜åœ¨ã‚’ã¾ãšç¢ºèª
                has_entries = hasattr(feed, "entries") and len(feed.entries) > 0
                logger.debug(
                    f"ã‚¨ãƒ³ãƒˆãƒªç¢ºèª: has_entries={has_entries}, ã‚¨ãƒ³ãƒˆãƒªæ•°={len(feed.entries) if hasattr(feed, 'entries') else 'N/A'}"
                )

                if feed.bozo:
                    logger.debug(
                        f"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ‘ãƒ¼ã‚¹ã§è»½å¾®ãªè­¦å‘Š ({url}): {feed.bozo_exception}"
                    )

                    # ã‚¨ãƒ³ãƒˆãƒªãŒã‚ã‚Œã°è­¦å‘ŠãŒã‚ã£ã¦ã‚‚å‡¦ç†ç¶šè¡Œ
                    if has_entries:
                        logger.info(
                            f"ãƒ‘ãƒ¼ã‚¹è­¦å‘ŠãŒã‚ã‚Šã¾ã™ãŒã€{len(feed.entries)}ä»¶ã®ã‚¨ãƒ³ãƒˆãƒªãŒå–å¾—ã§ããŸãŸã‚å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™"
                        )
                    else:
                        logger.warning(
                            f"ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã§ã‚¨ãƒ³ãƒˆãƒªãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {url}"
                        )
                        continue

                if not has_entries:
                    logger.warning("ãƒ•ã‚£ãƒ¼ãƒ‰ã«ã‚¨ãƒ³ãƒˆãƒªãŒã‚ã‚Šã¾ã›ã‚“")
                    continue

                logger.info(
                    f"ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—å®Œäº†: {len(feed.entries)}ä»¶ã®ã‚¨ãƒ³ãƒˆãƒª ({url})"
                )

                # ãƒ•ã‚£ãƒ¼ãƒ‰ã®åŸºæœ¬æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
                if hasattr(feed, "feed"):
                    feed_info = feed.feed
                    logger.debug(
                        f"ãƒ•ã‚£ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«: {feed_info.get('title', 'Unknown')}"
                    )
                    logger.debug(
                        f"ãƒ•ã‚£ãƒ¼ãƒ‰èª¬æ˜: {feed_info.get('description', 'Unknown')}"
                    )
                    logger.debug(f"æœ€çµ‚æ›´æ–°: {feed_info.get('updated', 'Unknown')}")

                return feed

            except Exception as e:
                logger.warning(f"ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ã‚¨ãƒ©ãƒ¼: {type(e).__name__}")
                continue

        # ã™ã¹ã¦ã®URLã§å¤±æ•—ã—ãŸå ´åˆ
        logger.error("ã™ã¹ã¦ã®RSS URLã§ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return None

    def parse_date(self, date_str: str) -> Optional[datetime]:
        """
        æ—¥ä»˜æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆè¤‡æ•°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œï¼‰

        Args:
            date_str: æ—¥ä»˜æ–‡å­—åˆ—

        Returns:
            datetime ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
        """
        if not date_str:
            return None

        try:
            # ã¾ãš feedparser ã®æ¨™æº–ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’è©¦è¡Œ
            time_struct = feedparser._parse_date(date_str)
            if time_struct:
                return datetime(*time_struct[:6], tzinfo=timezone.utc)
        except Exception as e:
            logger.debug(f"feedparser ã«ã‚ˆã‚‹æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {date_str} - {e}")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ä¸€èˆ¬çš„ãªæ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’è©¦è¡Œ
        date_formats = [
            "%Y-%m-%dT%H:%M:%S.%fZ",
            "%Y-%m-%dT%H:%M:%SZ",
            "%Y-%m-%dT%H:%M:%S",
            "%Y-%m-%d %H:%M:%S",
            "%a, %d %b %Y %H:%M:%S %z",
            "%a, %d %b %Y %H:%M:%S GMT",
            "%Y-%m-%dT%H:%M:%S%z",
        ]

        for fmt in date_formats:
            try:
                # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ãŒãªã„å ´åˆã¯UTCã¨ã—ã¦æ‰±ã†
                if "%z" in fmt:
                    parsed_dt = datetime.strptime(date_str, fmt)
                else:
                    parsed_dt = datetime.strptime(date_str, fmt).replace(
                        tzinfo=timezone.utc
                    )
                return parsed_dt
            except ValueError:
                continue

        logger.debug(f"ã™ã¹ã¦ã®æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è§£æã«å¤±æ•—: {date_str}")
        return None

    def filter_recent_updates(self, feed: feedparser.FeedParserDict) -> List[Dict]:
        """
        æŒ‡å®šæ™‚é–“å†…ã®æ›´æ–°ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

        Args:
            feed: ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰

        Returns:
            ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸæ›´æ–°ãƒªã‚¹ãƒˆ
        """
        recent_updates = []

        for entry in feed.entries:
            # å…¬é–‹æ—¥æ™‚ã‚’å–å¾—ï¼ˆAzure Updates ã§ã¯ a10:updated ã‚’å„ªå…ˆï¼‰
            published_date = None

            # 1. ã¾ãš a10:updated ã‚’ç¢ºèªï¼ˆAzure Updates ã®å®Ÿéš›ã®æ›´æ–°æ—¥æ™‚ï¼‰
            if hasattr(entry, "updated"):
                published_date = self.parse_date(entry.updated)
                logger.debug(
                    f"a10:updated ã‹ã‚‰æ—¥ä»˜å–å¾—: {entry.updated} -> {published_date}"
                )

            # 2. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: published ã‚’ç¢ºèª
            if not published_date and hasattr(entry, "published"):
                published_date = self.parse_date(entry.published)
                logger.debug(
                    f"published ã‹ã‚‰æ—¥ä»˜å–å¾—: {entry.published} -> {published_date}"
                )

            # 3. æœ€å¾Œã®æ‰‹æ®µ: updated_parsed ã‚’ç¢ºèª
            if (
                not published_date
                and hasattr(entry, "updated_parsed")
                and entry.updated_parsed
            ):
                try:
                    published_date = datetime(
                        *entry.updated_parsed[:6], tzinfo=timezone.utc
                    )
                    logger.debug(
                        f"updated_parsed ã‹ã‚‰æ—¥ä»˜å–å¾—: {entry.updated_parsed} -> {published_date}"
                    )
                except Exception as e:
                    logger.debug(f"updated_parsed ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")

            if not published_date:
                logger.debug(
                    f"æ—¥ä»˜ãŒå–å¾—ã§ããªã„ã‚¨ãƒ³ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—: {entry.get('title', 'Unknown')}"
                )
                continue

            # æŒ‡å®šæ™‚é–“å†…ã‹ãƒã‚§ãƒƒã‚¯
            if published_date >= self.cutoff_time:
                update_info = {
                    "title": entry.get("title", ""),
                    "description": entry.get("description", ""),
                    "link": entry.get("link", ""),
                    "published": published_date,
                    "categories": [
                        tag.get("term", "") for tag in entry.get("tags", [])
                    ],
                }
                recent_updates.append(update_info)
                logger.info(f"å¯¾è±¡æ›´æ–°ã‚’ç™ºè¦‹: {update_info['title']}")

        logger.info(f"{len(recent_updates)}ä»¶ã®æœ€æ–°æ›´æ–°ã‚’ç™ºè¦‹")
        return recent_updates

    def process_updates(self) -> List[Dict]:
        """
        æ›´æ–°ã‚’å‡¦ç†ã—ã¦è¦ç´„ã‚’ç”Ÿæˆ

        Returns:
            è¦ç´„ä»˜ãæ›´æ–°ãƒªã‚¹ãƒˆ
        """
        # RSS ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—
        feed = self.fetch_rss_feed()
        if not feed:
            return []

        # æœ€æ–°æ›´æ–°ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        recent_updates = self.filter_recent_updates(feed)
        if not recent_updates:
            logger.info("æŒ‡å®šæ™‚é–“å†…ã«æ–°ã—ã„æ›´æ–°ã¯ã‚ã‚Šã¾ã›ã‚“")
            return []

        # å„æ›´æ–°ã‚’è¦ç´„
        processed_updates = []
        for i, update in enumerate(recent_updates, 1):
            logger.info(f"æ›´æ–°ã‚’å‡¦ç†ä¸­ ({i}/{len(recent_updates)}): {update['title']}")

            # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å°‘ã—å¾…æ©Ÿ
            if i > 1:
                time.sleep(1)

            # è¦ç´„ç”Ÿæˆ
            summary = self.openai_client.summarize_update(
                update["title"], update["description"], update["link"]
            )

            update["summary"] = summary
            processed_updates.append(update)

            if summary:
                logger.info(f"è¦ç´„ç”Ÿæˆå®Œäº†: {update['title'][:50]}...")
            else:
                logger.warning(f"è¦ç´„ç”Ÿæˆå¤±æ•—: {update['title'][:50]}...")

        return processed_updates

    def generate_markdown_report(self, updates: List[Dict]) -> str:
        """
        ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ

        Args:
            updates: å‡¦ç†æ¸ˆã¿æ›´æ–°ãƒªã‚¹ãƒˆ

        Returns:
            ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆ
        """
        today = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
        report_lines = [
            f"# {today} - Azure Updates è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆ",
            f"",
            f"**ç”Ÿæˆæ—¥æ™‚**: {today}",
            f"**å¯¾è±¡æœŸé–“**: éå» {self.check_hours} æ™‚é–“ä»¥å†…",
            f"**æ›´æ–°ä»¶æ•°**: {len(updates)} ä»¶",
            f"",
        ]

        if not updates:
            report_lines.extend(
                [
                    "## çµæœ",
                    "",
                    "æœ¬æ—¥ã®æ›´æ–°ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
                    "",
                ]
            )
        else:
            report_lines.extend(["## æ›´æ–°ä¸€è¦§", ""])

            for i, update in enumerate(updates, 1):
                report_lines.extend(
                    [
                        f"### {i}. {update['title']}",
                        "",
                        f"**å…¬é–‹æ—¥æ™‚**: {update['published'].strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S UTC')}",
                        f"**ãƒªãƒ³ã‚¯**: [{update['title']}]({update['link']})",
                        "",
                    ]
                )

                if update["categories"]:
                    categories_str = ", ".join(update["categories"])
                    report_lines.extend([f"**ã‚«ãƒ†ã‚´ãƒª**: {categories_str}", ""])

                if update.get("summary"):
                    report_lines.extend(["**è¦ç´„**:", "", update["summary"], ""])
                else:
                    report_lines.extend(["**è¦ç´„**: ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ", ""])

                report_lines.extend(
                    ["**è©³ç´°**:", "", update["description"], "", "---", ""]
                )

        report_lines.extend(
            [
                "",
                f"*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ - {datetime.now(timezone(timedelta(hours=9))).strftime('%Y-%m-%d %H:%M:%S JST')}*",
            ]
        )

        return "\n".join(report_lines)

    def save_report(self, content: str, output_dir: str = "updates") -> str:
        """
        ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

        Args:
            content: ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

        Returns:
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        os.makedirs(output_dir, exist_ok=True)

        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆï¼ˆæœ¬æ—¥ã®æ—¥ä»˜ï¼‰
        today = datetime.now().strftime("%Y-%m-%d")
        filename = f"azure-updates-{today}.md"
        filepath = os.path.join(output_dir, filename)

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        try:
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(content)
            logger.info(f"ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
            return filepath
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            raise


def load_config() -> Dict[str, str]:
    """
    ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿

    Returns:
        è¨­å®šè¾æ›¸
    """
    config = {
        "endpoint": os.getenv("AOAI_ENDPOINT"),
        "api_key": os.getenv("AOAI_KEY"),
        "deployment": os.getenv("DEPLOYMENT"),
        "api_version": os.getenv("API_VER", "2024-02-15-preview"),
        "check_hours": int(os.getenv("CHECK_HOURS", "24")),
    }

    # å¿…é ˆè¨­å®šã®æ¤œè¨¼
    required_vars = ["endpoint", "api_key", "deployment"]
    missing_vars = [var for var in required_vars if not config[var]]

    if missing_vars:
        logger.error(
            f"å¿…é ˆç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {', '.join(missing_vars.upper())}"
        )
        sys.exit(1)

    logger.info("è¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    return config


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(description="Azure Updates RSS ãƒ•ã‚£ãƒ¼ãƒ‰å‡¦ç†")
    parser.add_argument(
        "--hours",
        type=int,
        help="ãƒã‚§ãƒƒã‚¯å¯¾è±¡æ™‚é–“ï¼ˆæ™‚é–“ï¼‰ã€‚ç’°å¢ƒå¤‰æ•° CHECK_HOURS ã‚ˆã‚Šå„ªå…ˆã•ã‚Œã¾ã™",
    )
    parser.add_argument(
        "--output-dir",
        default="updates",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: updatesï¼‰",
    )
    parser.add_argument("--verbose", action="store_true", help="è©³ç´°ãƒ­ã‚°ã‚’å‡ºåŠ›")
    parser.add_argument(
        "--test-feed", action="store_true", help="RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®ãƒ†ã‚¹ãƒˆå–å¾—ã®ã¿å®Ÿè¡Œ"
    )

    args = parser.parse_args()

    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    try:
        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
        if args.test_feed:
            logger.info("RSSãƒ•ã‚£ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­...")
            processor = AzureUpdatesProcessor(None, 24)  # OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãªã—
            feed = processor.fetch_rss_feed()
            if feed:
                print(f"\nâœ… ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—æˆåŠŸ!")
                print(f"ğŸ“Š ã‚¨ãƒ³ãƒˆãƒªæ•°: {len(feed.entries)} ä»¶")

                # ãƒ•ã‚£ãƒ¼ãƒ‰æƒ…å ±è¡¨ç¤º
                if hasattr(feed, "feed"):
                    feed_info = feed.feed
                    print(f"ğŸ“° ãƒ•ã‚£ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«: {feed_info.get('title', 'Unknown')}")
                    print(
                        f"ğŸ“ ãƒ•ã‚£ãƒ¼ãƒ‰èª¬æ˜: {feed_info.get('description', 'Unknown')[:100]}..."
                    )

                if len(feed.entries) > 0:
                    print(f"\nğŸ“‹ æœ€æ–°ã‚¨ãƒ³ãƒˆãƒªä¾‹ï¼ˆæœ€å¤§5ä»¶ï¼‰:")
                    for i, entry in enumerate(feed.entries[:5], 1):
                        title = entry.get("title", "No Title")
                        # Azure Updates ã§ã¯ updated ã‚’å„ªå…ˆ
                        updated = entry.get(
                            "updated", entry.get("published", "No Date")
                        )
                        link = entry.get("link", "No Link")

                        print(f"  {i}. {title[:80]}...")
                        print(f"     ğŸ“… æ›´æ–°æ—¥: {updated}")
                        print(f"     ğŸ”— ãƒªãƒ³ã‚¯: {link}")

                        # æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ
                        parsed_date = processor.parse_date(updated)
                        if parsed_date:
                            print(f"     âœ… æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹æˆåŠŸ: {parsed_date}")
                        else:
                            print(f"     âŒ æ—¥ä»˜ãƒ‘ãƒ¼ã‚¹å¤±æ•—")
                        print()

                print(f"ğŸ‰ ãƒ†ã‚¹ãƒˆå®Œäº†: RSSãƒ•ã‚£ãƒ¼ãƒ‰ã¯æ­£å¸¸ã«å–å¾—ãƒ»ãƒ‘ãƒ¼ã‚¹ã§ãã¦ã„ã¾ã™")
            else:
                print("\nâŒ ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—å¤±æ•—")
                print("ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ« azure_updates.log ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            return

        # è¨­å®šèª­ã¿è¾¼ã¿
        config = load_config()

        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æ™‚é–“ã‚’ä¸Šæ›¸ã
        if args.hours:
            config["check_hours"] = args.hours

        # Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        openai_client = AzureOpenAIClient(
            endpoint=config["endpoint"],
            api_key=config["api_key"],
            deployment=config["deployment"],
            api_version=config["api_version"],
        )

        # Azure Updates ãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–
        processor = AzureUpdatesProcessor(
            openai_client=openai_client, check_hours=config["check_hours"]
        )

        # æ›´æ–°å‡¦ç†
        logger.info("Azure Updates ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
        updates = processor.process_updates()

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        logger.info("ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­")
        report_content = processor.generate_markdown_report(updates)

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        output_path = processor.save_report(report_content, args.output_dir)

        # çµæœè¡¨ç¤º
        print(f"\nâœ… å‡¦ç†å®Œäº†!")
        print(f"ğŸ“Š å‡¦ç†ä»¶æ•°: {len(updates)} ä»¶")
        print(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")

        if len(updates) > 0:
            print(f"\nğŸ“‹ æ›´æ–°ä¸€è¦§:")
            for i, update in enumerate(updates, 1):
                status = "âœ…" if update.get("summary") else "âŒ"
                print(f"  {i}. {status} {update['title'][:60]}...")

        logger.info("å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")

    except KeyboardInterrupt:
        logger.info("å‡¦ç†ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(130)
    except Exception as e:
        logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
